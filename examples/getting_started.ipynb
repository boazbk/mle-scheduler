{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f9d077-64ed-4dc3-b772-498569e96d2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `mle-scheduler`: Lightweight Cluster/Cloud Job Scheduling 🚂\n",
    "### Author: [@RobertTLange](https://twitter.com/RobertTLange) [Last Update: November 2021][![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mle-infrastructure/mle-scheduler/blob/main/examples/getting_started.ipynb)\n",
    "\n",
    "\"How does one specify the amount of required CPU cores and GPU type again?\" I dislike having to write cluster job submission files. It is tedious, I always forget something and copying old templates feels cumbersome. The classic boilerplate problem. What if instead there was a tool that would completely get rid of this manual work? **A tool that schedules jobs on different cluster/cloud resources** with minimal and intuitive job/resource description requirements? \n",
    "\n",
    "The `mle-scheduler` package aims to provide such a service for [Slurm](https://slurm.schedmd.com/), [Open Grid Engine](http://gridscheduler.sourceforge.net/documentation.html) cluster schedulers & [Google Cloud Platform VMs](https://cloud.google.com/gcp/). But you can also use the scheduling utilities to launch multiple training runs on a local machine or a set of SSH servers. It comes with two core functional pillars:\n",
    "\n",
    "- **`MLEJob`**: Launches and monitors a single job on a resource (Slurm, Open Grid Engine, GCP, SSH, etc.).\n",
    "- **`MLEQueue`**: Launches and monitors a queue of jobs with different training configurations and/or random seeds.\n",
    "\n",
    "In the remainder of this walkthrough we will first introduce the general logic locally and afterwards go through how to launch and monitor jobs on the different clusters, SSH servers and GCP VMs. Importantly, the `mle-scheduler` is part of the broader [`mle-infrastructure`](https://github.com/mle-infrastructure) and comes with a set of handy built-in synergies. We will wrap-up by highlighting some of these.\n",
    "\n",
    "![](../docs/mle_scheduler_structure.png)\n",
    "![](https://github.com/mle-infrastructure/mle-scheduler/blob/main/docs/mle_scheduler_structure.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ff5d35-b29e-4386-b423-9c83ad1ae169",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "try:\n",
    "    import mle_scheduler\n",
    "except:\n",
    "    !pip install -q mle-scheduler\n",
    "    \n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d0799-a069-48ad-a8c7-0f06f5a239c6",
   "metadata": {},
   "source": [
    "Let's start by setting up a simple training script. In the code snippet below we \"pseudo\"-train a network for 10 epochs and log the training run statistics using [`mle-logging`](https://github.com/mle-infrastructure/mle-logging). I use it on a daily basis and throughout my experiments - but you can use whatever logging setup you prefer. Our execution script takes three command line arguments that are being parsed at the start-up: `experiment_dir` (the main log directory), `config_fname` (the configuration file to be loaded), `seed_id` (the random number seed for reproduction). The black-box training loop then runs for some epochs and continuously logs some data to our logger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cfa9ca-559b-487e-880e-de865279b353",
   "metadata": {},
   "source": [
    "```python\n",
    "from mle_logging import MLELogger, load_config\n",
    "\n",
    "\n",
    "def main(experiment_dir: str, config_fname: str, seed_id: int):\n",
    "    \"\"\"Example training 'loop' using MLE-Logging.\"\"\"\n",
    "    # Load experiment configuration & setup logging to experiment_dir\n",
    "    train_config = load_config(config_fname)\n",
    "    log = MLELogger(\n",
    "        experiment_dir=experiment_dir,\n",
    "        config_fname=config_fname,\n",
    "        seed_id=seed_id,\n",
    "        time_to_track=[\"num_epochs\"],\n",
    "        what_to_track=[\"train_loss\", \"test_loss\"],\n",
    "    )\n",
    "    # 'Run' the pseudo training loop and feed the logger\n",
    "    for epoch in range(1, 11):\n",
    "        train_loss, test_loss = train_your_net(epoch, seed_id, **train_config)\n",
    "        log.update(\n",
    "            {\"num_epochs\": epoch},\n",
    "            {\"train_loss\": train_loss, \"test_loss\": test_loss},\n",
    "            save=True,\n",
    "        )\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Let's train a network.\")\n",
    "    parser.add_argument(\"-exp_dir\", \"--experiment_dir\", type=str)\n",
    "    parser.add_argument(\"-config\", \"--config_fname\", type=str)\n",
    "    parser.add_argument(\"-seed\", \"--seed_id\", type=int, default=1)\n",
    "    args = vars(parser.parse_args())\n",
    "    main(args[\"experiment_dir\"], args[\"config_fname\"], args[\"seed_id\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b0522-6c4b-440d-8dca-345ba9e8a461",
   "metadata": {},
   "source": [
    "# Single Job Management with `MLEJob` 🚀\n",
    "\n",
    "So now that we have defined the script which we would like to execute, the next natural step would be we simply run it? The most natural thing to do is to simply execute the python script via:\n",
    "\n",
    "```\n",
    "python train.py -config base_config_1.yaml -exp_dir logs_single -seed_id 1\n",
    "```\n",
    "\n",
    "The same can be achieved programmatically on our local machine using an instance of `MLEJob`. In the cell below you can see how to spawn a process which executes the exact same python command and continuously monitors it until the job is completed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a221b5-758b-491b-b206-65c3bddeeddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:34:29] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> PID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">19044</span> - Local job scheduled - base_config_1.yaml          <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:146</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8a806be880>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> PID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">19044</span> - Started monitoring - base_config_1.yaml           <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:208</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8a88665250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:34:32] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> PID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">19044</span> - Local job completed - base_config_1.yaml          <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:213</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8a806bea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mle_scheduler import MLEJob\n",
    "\n",
    "job = MLEJob(resource_to_run=\"local\",\n",
    "             job_filename=\"train.py\",\n",
    "             config_filename=\"base_config_1.yaml\",\n",
    "             experiment_dir=\"logs_single\",\n",
    "             seed_id=1,\n",
    "             logger_level=logging.INFO)\n",
    "\n",
    "_ = job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ca15b-c9a6-4807-8e81-2cc523369a58",
   "metadata": {
    "tags": []
   },
   "source": [
    "`config_filename`, `experiment_dir` and `seed_id` are three command line arguments, which I found myself to be using fairly often. But they are not at all necessary for job execution via the `MLEJob`. Indeed, you can provide a `extra_cmd_line_input` dictionary at job instance creation, which specifies key, value pairs for `-<key> <value>`-style arguments. E.g.\n",
    "\n",
    "```python\n",
    "MLEJob(\n",
    "    resource_to_run=\"local\",\n",
    "    job_filename=\"train.py\",\n",
    "    extra_cmd_line_input={\"optim\": \"Adam\", \"lrate\": 0.1},\n",
    ")\n",
    "```\n",
    "\n",
    "will launch `python train.py -optim Adam -lrate -0.1`. Furthermore, you can provide a set of `job_arguments` to execute the job in a previously created virtual environment of your choice:\n",
    "\n",
    "```python\n",
    "job = MLEJob(resource_to_run=\"local\",\n",
    "             job_filename=\"train.py\",\n",
    "             job_arguments={\"use_conda_venv\": True,\n",
    "                            \"use_venv_venv\": False,\n",
    "                            \"env_name\": \"mle-toolbox\"}\n",
    "             config_filename=\"base_config_1.yaml\",\n",
    "             experiment_dir=\"logs_single\",\n",
    "             seed_id=1,\n",
    "             logger_level=logging.INFO)\n",
    "```\n",
    "\n",
    "This will activate a conda virtual environment named `mle-toolbox` before executing the Python script. Finally, instead of automatically monitoring the job until completion, you can also manually monitor the status of a previously scheduled job by combining `job.schedule` and `job.monitor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e191f835-c110-42f8-bbec-44857399fa1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:34:36] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> PID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">19047</span> - Local job scheduled - base_config_1.yaml          <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:146</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8a806e9910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> PID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">19047</span> - Started monitoring - base_config_1.yaml           <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:208</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8a806e9ac0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:34:39] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> PID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">19047</span> - Local job completed - base_config_1.yaml          <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:213</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8a806e9ac0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "job_id = job.schedule()\n",
    "while True:\n",
    "    status = job.monitor(job_id)\n",
    "    if status == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74100d5-a957-4a7e-ae87-4a4626babdc4",
   "metadata": {},
   "source": [
    "# Job Queue Management with `MLEQueue` 🚀,...,🚀\n",
    "\n",
    "Next up we can also launch multiple runs for different configurations and random seeds in a queue, that will execute them simultaneously using the `MLEQueue`. The queue simply wraps around `MLEJob` and will by default run and monitor the jobs multiple all at once. The following code will execute 4 runs for two configurations (`base_config_1.yaml` and `base_config_2.yaml`) with two different random seeds (`0` and `1`) alà:\n",
    "\n",
    "```\n",
    "python train.py -config base_config_1.yaml -seed 0 -exp_dir logs_queue/<date>_base_config_1\n",
    "python train.py -config base_config_1.yaml -seed 1 -exp_dir logs_queue/<date>_base_config_1\n",
    "python train.py -config base_config_2.yaml -seed 0 -exp_dir logs_queue/<date>_base_config_2\n",
    "python train.py -config base_config_2.yaml -seed 1 -exp_dir logs_queue/<date>_base_config_2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed9e8a7-74fd-4725-9bd6-31106018069d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704709ae275240b7bbfcbcb54aad9f61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mle_scheduler import MLEQueue\n",
    "\n",
    "queue = MLEQueue(resource_to_run=\"local\",\n",
    "                 job_filename=\"train.py\",\n",
    "                 config_filenames=[\"base_config_1.yaml\",\n",
    "                                   \"base_config_2.yaml\"],\n",
    "                 random_seeds=[0, 1],\n",
    "                 experiment_dir=\"logs_queue\")\n",
    "queue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1587b0-79cb-458b-b4b6-b6d062ade051",
   "metadata": {},
   "source": [
    "By default all jobs will be launched/queued in parallel. But you can also control how many jobs to schedule at a given point in time. E.g. if you don't want more than 10 jobs to be running/queued simultaneously simply add the `max_running_jobs=<num_jobs>` option to the `MLEQueue`. In this case a new job will be launched, whenever a previously queued job terminates and there do remain unscheduled ones left in the queue.\n",
    "\n",
    "Above we have explicitly provided the random seeds we would like to run our experiment on. Alternatively, if you would like to randomly generate these seeds, you can also simply specify the number of desired seeds via the `num_seeds` option. And again, you can use `job_arguments` to specify a virtual environment or different command line arguments via `extra_cmd_line_input`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77fef46-4d6c-41e0-80a9-55a823356135",
   "metadata": {},
   "source": [
    "# Launching a Slurm/GridEngine Cluster-based Job\n",
    "\n",
    "All the previous code runs perfectly fine on a single local machine (e.g. the Colab/notebook server you are executing these cells on). But the true power of the `mle-scheduler` lies in its capabilities to run jobs on multiple remote compute nodes such as high-performance clusters (HPC). At the point of writing the `mle-scheduler` implements job launch/monitor utilities for both [Slurm](https://slurm.schedmd.com/) and [Open Grid Engine](http://gridscheduler.sourceforge.net/documentation.html) schedulers.\n",
    "\n",
    "So how can you specify the resources that you would like to request when scheduling a job? This happens through the `job_arguments` input dictionary. For both cluster schedulers, you have the following resource options: \n",
    "\n",
    "|*All Schedulers*| Type | Optional | Description | \n",
    "|----|:----: | :----: | ----|\n",
    "|`num_logical_cores` | `int` | ❌ | Number of logical cores requested per job\n",
    "|`num_gpus` | `int` | ✔️ | Number of GPUs requested per job\n",
    "|`gpu_type` | `str` | ✔️ | Type of GPU requested per job\n",
    "|`memory_per_job` | `int` | ✔️ | Memory in MB requested per job\n",
    "|`time_per_job` | `str` | ✔️ | Max. job run time (`dd:hh:mm`) before termination\n",
    "|`env_name` | `str` | ✔️ | Virtual environment name to activate at job start-up\n",
    "|`use_conda_venv` | `bool` | ✔️ | Whether to use a `virtualenv`-based environment\n",
    "|`use_venv_venv` | `bool` | ✔️ | Whether to use an Anaconda-based environment\n",
    "|`job_name` | `str` | ✔️ | Job name of scheduled job shown in (`squeue`/`qstat`)\n",
    "\n",
    "For Slurm schedulers you have the following additional arguments:\n",
    "\n",
    "|*Only Slurm*| Type | Optional | Description | \n",
    "|----|:----: | :----: | ----|\n",
    "|`partition` | `str` | ❌ | Partition to schedule job on\n",
    "|`modules_to_load` | `Union[List[str], str]` | ✔️ | Modules to load at job start-up\n",
    "\n",
    "And for Open Grid Engine schedulers:\n",
    "\n",
    "|*Only Grid Engine* | Type | Optional | Description | \n",
    "|----|:----: | :----: | ----|\n",
    "|`queue` | `str` | ❌ | Queue to schedule job on\n",
    "|`gpu_prefix` | `str` | ✔️ | Prefix to request GPU nodes (`#$ -l {gpu_prefix}=\"{num_gpus}\"`)\n",
    "|`exclude_nodes` | `List[str]` | ✔️ | List of node addresses to exclude from being scheduled on\n",
    "\n",
    "Importantly, when launching a `MLEJob`/`MLEQueue` we assume that you are currently on the headnode of your cluster which uses a standard shared file system. [In a future release I plan on adding the option to launch cluster jobs from your local machine - so stay tuned!] Otherwise everything else remains the same as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bf46e2a-d35d-4b2c-97ba-14c22132dc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "echo \"------------------------------------------------------------------------\"\n",
      "source ~/miniconda3/etc/profile.d/conda.sh\n",
      "echo \"------------------------------------------------------------------------\"\n",
      ". ~/.bashrc && conda activate {env_name}\n",
      "echo \"Successfully activated virtual environment - Ready to start job\"\n",
      "\n",
      "b'' b'/bin/sh: sbatch: command not found\\n'\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'squeue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/y5/n8jxv2j53tb33c7f76r2prvc0000gn/T/ipykernel_19036/2589327758.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mrandom_seeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job_queue.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# 1. Spawn 1st batch of evals until limit of allowed usage is reached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_running_jobs\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_running_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_total_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m             \u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue_counter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"status\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue_counter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"job\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job_queue.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self, queue_counter)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;31m# 2. Launch a single experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;31m# 3. Return updated counter, `Job` instance & corresponding ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\u001b[0m in \u001b[0;36mschedule\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;34m\"\"\"Schedule job on cluster (sge/slurm), cloud (gcp) or locally.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_to_run\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcluster_resources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_status\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 self.logger.info(\n",
      "\u001b[0;32m~/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\u001b[0m in \u001b[0;36mschedule_cluster\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m             )\n\u001b[1;32m    257\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_to_run\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"slurm-cluster\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             job_id = submit_slurm(\n\u001b[0m\u001b[1;32m    259\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcmd_line_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/cluster/slurm/manage_slurm.py\u001b[0m in \u001b[0;36msubmit_slurm\u001b[0;34m(filename, cmd_line_arguments, job_arguments, user_name, clean_up)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;31m# Wait until the job is listed under the qstat scheduled jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mjob_running\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonitor_slurm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mjob_running\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/cluster/slurm/manage_slurm.py\u001b[0m in \u001b[0;36mmonitor_slurm\u001b[0;34m(job_id, user_name)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"squeue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-u\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mle-toolbox/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    425\u001b[0m                **kwargs).stdout\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mle-toolbox/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'stderr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mle-toolbox/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/mle-toolbox/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1819\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'squeue'"
     ]
    }
   ],
   "source": [
    "# Each job requests 5 CPU cores & 1 V100S GPU w. CUDA 10.0 loaded\n",
    "job_args = {\"partition\": \"standard\",\n",
    "            \"env_name\": \"mle-toolbox\",\n",
    "            \"use_conda_venv\": True,\n",
    "            \"num_logical_cores\": 5,\n",
    "            \"num_gpus\": 1,\n",
    "            \"gpu_type\": \"V100S\",\n",
    "            \"modules_to_load\": \"nvidia/cuda/10.0\",\n",
    "            \"job_name\": \"test\"}\n",
    "\n",
    "queue = MLEQueue(\n",
    "    resource_to_run=\"slurm-cluster\",\n",
    "    job_filename=\"train.py\",\n",
    "    job_arguments=job_args,\n",
    "    config_filenames=[\"base_config_1.yaml\", \"base_config_2.yaml\"],\n",
    "    experiment_dir=\"logs_queue_slurm\",\n",
    "    random_seeds=[0, 1]\n",
    ")\n",
    "queue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcbc8be-42d4-4fa0-b5be-0c614c4fcccb",
   "metadata": {},
   "source": [
    "# Launching an SSH Server-based Job\n",
    "\n",
    "Not every lab may have access to an institution-wide HPC. Instead you may be working with a set of servers, which you can access via SSH. The `mle-scheduler` also allows you to launch jobs in such situations. In order to do so, you will first need to have setup passwordless SSH access to your. This can for example be done by following [these](https://linuxize.com/post/how-to-setup-passwordless-ssh-login/) instructions and will create a private key on your local machine (e.g. a file called `~.ssh/id_rsa`). You will have to provide this key path as well as your user name, server address, SSH port and the remote code directory in the `ssh_settings` which will be passed to the `MLEJob` & `MLEQueue` instances. This will allow us to establish a SSH connection using the [`paramiko`](https://github.com/paramiko/paramiko) client.\n",
    "\n",
    "All SSH server-based jobs have to provide the `ssh_settings` input to `MLEJob` and `MLEQueue`.\n",
    "\n",
    "In order to launch a job from your local machine on the remote server you will most likely first need to copy the code directory to the server. This can either be done manually using `scp` or using the `send_code_ssh` function. By default this will send the current working directory to `ssh_settings[\"remote_dir\"]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "713854c7-6a45-4e9f-ada7-936c08862ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mle_scheduler.ssh import send_dir_ssh, copy_dir_ssh, delete_dir_ssh\n",
    "\n",
    "ssh_settings = {\"user_name\":\"RobTLange\",\n",
    "                \"pkey_path\": '~.ssh/id_rsa',\n",
    "                \"main_server\": 'cluster.ml.tu-berlin.de',\n",
    "                \"jump_server\": '',\n",
    "                \"ssh_port\": 22,\n",
    "                \"remote_dir\": \"mle-code-dir\"}\n",
    "\n",
    "# scp the current working directory to a mle-code-dir remote\n",
    "send_dir_ssh(ssh_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f9e3b5-3073-4380-8b31-a54971fd00a8",
   "metadata": {},
   "source": [
    "Afterwards, we are ready to run the job on the SSH server by `ssh_settings` to our `MLEJob` instance. Once the job is completed the results can be copied back to your local machine using `copy_results_ssh` and the directory path storing the results. Finally, you can use `clean_up_ssh` to delete the previously copied code directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6f54dab-dda0-45db-9992-49dbcedd65d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.py  -exp_dir logs_ssh_single -config base_config_1.yaml {'env_name': 'mle-toolbox', 'use_conda_venv': True} {'user_name': 'RobTLange', 'pkey_path': '~.ssh/id_rsa', 'main_server': 'cluster.ml.tu-berlin.de', 'jump_server': '', 'ssh_port': 22, 'remote_dir': 'mle-code-dir'}\n",
      "echo $$; /bin/bash -c 'source $(conda info --base)/etc/profile.d/conda.sh && conda activate mle-toolbox && cd mle-code-dir && python train.py  -exp_dir logs_ssh_single -config base_config_1.yaml'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:38:37] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> PID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">3052434</span> - SSH job scheduled - base_config_1.yaml          <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:158</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8aa8cb2370>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> SSH PID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">3052434</span> - Started monitoring - base_config_1.yaml     <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:198</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8a681099d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:38:43] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> SSH PID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">3052434</span> - SSH job completed - base_config_1.yaml      <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:203</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8a681099d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "job = MLEJob(resource_to_run=\"ssh-node\",\n",
    "             job_filename=\"train.py\",\n",
    "             config_filename=\"base_config_1.yaml\",\n",
    "             experiment_dir=\"logs_ssh_single\",\n",
    "             job_arguments={\"env_name\": \"mle-toolbox\",\n",
    "                            \"use_conda_venv\": True},\n",
    "             ssh_settings=ssh_settings,\n",
    "             logger_level=logging.INFO)\n",
    "\n",
    "job.run()\n",
    "\n",
    "# Copy over the results from the SSH server\n",
    "copy_dir_ssh(ssh_settings, remote_dir=\"mle-code-dir/logs_ssh_single\")\n",
    "\n",
    "# Delete the code directory on the SSH server\n",
    "delete_dir_ssh(ssh_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11814eb8-3c04-46fa-8967-91f6e599d1f5",
   "metadata": {},
   "source": [
    "If you would like to run a queue of jobs, you don't want to copy the same code directory over to the server each time. Instead, the `MLEQueue` will copy the directory (if desired) once at start-up and will delete it again once all jobs have terminated. Furthermore, it will automatically copy the results in to your local execution directory. Note that the error log messages don't have to bother us. They simply come from the ssh client needing a couple of attempts to establish a connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c99aa0c-461d-4ff6-8c59-cd88b0b2e187",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssh_settings[\"start_up_copy_dir\"] = True\n",
    "ssh_settings[\"clean_up_remote_dir\"] = True\n",
    "\n",
    "queue = MLEQueue(resource_to_run=\"ssh-node\",\n",
    "                 job_filename=\"train.py\",\n",
    "                 config_filenames=[\"base_config_1.yaml\",\n",
    "                                   \"base_config_2.yaml\"],\n",
    "                 random_seeds=[0, 1],\n",
    "                 experiment_dir=\"logs_ssh_queue\",\n",
    "                 job_arguments={\"env_name\": \"mle-toolbox\",\n",
    "                                \"use_conda_venv\": True},\n",
    "                 ssh_settings=ssh_settings)\n",
    "\n",
    "queue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104501cd-a40c-4409-8deb-75d6514647e8",
   "metadata": {},
   "source": [
    "# Launching a GCP VM-based Job\n",
    "\n",
    "Finally, let's talk about how to create GCP VM-based jobs using the `mle-scheduler`. This will require you to have already setup Google Cloud SDK and to have created a GCP project and a GCS bucket. Furthermore you will have to provide you `.json` authentication key path. If you don't have one yet, have a look [here](https://cloud.google.com/docs/authentication/getting-started). Alternatively, just make sure that the environment variable `GOOGLE_APPLICATION_CREDENTIALS` is set to the right path.\n",
    "\n",
    "As for the SSH server jobs we start by copying the code repository in a GCS bucket. In order to know where to sync these files to, you will need to provide the `project_name`, `bucket_name` and the name of the code directory in the bucket. Note that later on all generated results logs will also be stored in this GCS `remote_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c62df95a-bc3d-4609-848e-b8455f81255f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility to copy local code directory to GCS bucket\n",
    "from mle_scheduler.cloud.gcp import send_dir_gcp, copy_dir_gcp, delete_dir_gcp\n",
    "\n",
    "cloud_settings = {\n",
    "        \"project_name\": \"mle-toolbox\",\n",
    "        \"bucket_name\": \"mle-protocol\",\n",
    "        \"remote_dir\": \"mle-code-dir\"\n",
    "    }\n",
    "\n",
    "# Send config file to remote machine - independent of code base!\n",
    "send_dir_gcp(cloud_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3244832d-4076-4ebe-9f05-3e64a3c82f94",
   "metadata": {},
   "source": [
    "At VM startup each of the individual GCP VM instances will `gsync` copy the code and install a `requirements.txt` file in a fresh Python virtual environment. By default the created VMs are [`n1-high-cpu` custom VMs](https://cloud.google.com/compute/docs/general-purpose-machines#n1-high-cpu) since they allow for GPU attachment. The `N1` VMs allow for 1, 2, 4, 8, 16, 32 & 48 logical cores (x2 with hyperthreading) and up to 624 GB of memory. Right now the GCP `MLEJob` will also by default setup a `c1-deeplearning-tf-2-4-cu110-v20210414-debian-10` image from the `ml-images` family, which comes with CUDA 11.0 drivers ready to go. In the future I plan on adding more flexibility to specify resource requests through the `mle-scheduler` and to support TPU VMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afd88a1-95d5-4dae-9f2a-a42ff9d74fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gcloud', 'compute', 'instances', 'create', 'test-20211111-154942-9928', '--preemptible', '--zone=us-west1-a', '--custom-cpu=2', '--custom-memory=6144MB', '--custom-vm-type=n1', '--image=c1-deeplearning-tf-2-4-cu110-v20210414-debian-10', '--image-project=ml-images', '--metadata-from-file=startup-script=test-20211111-154942-9928-startup.sh', '--scopes=cloud-platform,storage-full', '--boot-disk-size=128GB', '--boot-disk-type=pd-standard', '--no-user-output-enabled', '--verbosity', 'error']\n",
      "{'ZONE': 'us-west1-a', 'ACCELERATOR_TYPE': None, 'ACCELERATOR_COUNT': 0, 'MACHINE_TYPE': 'n2-highcpu-8', 'IMAGE_NAME': 'c1-deeplearning-tf-2-4-cu110-v20210414-debian-10', 'IMAGE_PROJECT': 'ml-images'}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:49:55] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> VM Name: test-<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">20211111</span>-<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">154942</span>-<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">9928</span> - Cloud job scheduled -     <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:134</span>\n",
       "                    base_config_1.yaml                                                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8a68118a30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> VM Name: test-<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">20211111</span>-<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">154942</span>-<span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">9928</span> - Started monitoring -      <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:188</span>\n",
       "                    base_config_1.yaml                                                       \n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7f8a681185e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "job_args = {\n",
    "    \"use_tpus\": 0,\n",
    "    \"num_gpus\": 0,\n",
    "    \"gpu_type\": \"nvidia-tesla-v100\",\n",
    "    \"num_logical_cores\": 1,\n",
    "    \"job_name\": \"test\"\n",
    "}\n",
    "\n",
    "job = MLEJob(\n",
    "    resource_to_run=\"gcp-cloud\",\n",
    "    job_filename=\"train.py\",\n",
    "    config_filename=\"base_config_1.yaml\",\n",
    "    experiment_dir=\"logs_gcp_single\",\n",
    "    job_arguments=job_args,\n",
    "    cloud_settings=cloud_settings,\n",
    "    logger_level=logging.INFO\n",
    ")\n",
    "\n",
    "job.run()\n",
    "\n",
    "# Copy over the results from the SSH server\n",
    "copy_dir_gcp(cloud_settings, remote_dir=\"mle-code-dir/logs_gcp_single\")\n",
    "\n",
    "# Delete the code directory on the SSH server\n",
    "delete_dir_gcp(cloud_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a83178-3837-4c3b-a90a-4c491c007401",
   "metadata": {},
   "source": [
    "The `MLEJob` instance will create a GCP VM using the SDK syntax and automatically execute a command which looks like this:\n",
    "\n",
    "```\n",
    "gcloud compute instances create <job_name> --preemptible --zone=us-west1-a \\\n",
    "    --custom-cpu=2 \\\n",
    "    --custom-memory=6144MB \\\n",
    "    --custom-vm-type=n1 \\\n",
    "    --image=c1-deeplearning-tf-2-4-cu110-v20210414-debian-10 \\\n",
    "    --image-project=ml-images \\\n",
    "    --metadata-from-file=startup-script=<job_name>-<datetime>-startup.sh \\\n",
    "    --scopes=cloud-platform,storage-full \\\n",
    "    --boot-disk-size=128GB \\\n",
    "    --boot-disk-type=pd-standard \\\n",
    "    --no-user-output-enabled \\\n",
    "    --verbosity error\n",
    "```\n",
    "\n",
    "Again, for a single job you can copy over the results and delete the code stored in GCS bucket using `copy_dir_gcp` and `delete_dir_gcp`. \n",
    "\n",
    "The `MLEQueue` works as before for the SSH server setting and will take care of copying and cleaning up the code directory once all jobs have terminated successfully. Note that spawning many jobs simultaneously may be prohibited by your personal resource quota."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6f48c6-d4fc-4888-b7a1-f0c6a8742a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cloud_settings[\"start_up_copy_dir\"] = True\n",
    "cloud_settings[\"clean_up_remote_dir\"] = True\n",
    "\n",
    "queue = MLEQueue(\n",
    "    resource_to_run=\"gcp-cloud\",\n",
    "    job_filename=\"train.py\",\n",
    "    config_filenames=[\"base_config_1.yaml\", \"base_config_2.yaml\"],\n",
    "    random_seeds=[0, 1],\n",
    "    experiment_dir=\"logs_gcp_queue\",\n",
    "    job_arguments=job_args,\n",
    "    cloud_settings=cloud_settings,\n",
    ")\n",
    "queue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62303fa-2961-423c-ac52-22c19e9deeea",
   "metadata": {},
   "source": [
    "# Slack Notification for Real-Time Queue Status\n",
    "\n",
    "You can also get a Slack Bot notification of the current status of the queue. This is powered by the [`slack-clusterbot`](https://github.com/sprekelerlab/slack-clusterbot) package and you can follow these instructions to setup a bot for your favorite workspace [here](https://github.com/sprekelerlab/slack-clusterbot/wiki/Installation). After you have obtained your authentication token it is as simple as providing the token and your user name as inputs. Below you can find the slack chat output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26418d40-b771-4ed1-ba50-0e5826bcfc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "queue = MLEQueue(resource_to_run=\"local\",\n",
    "                 job_filename=\"train.py\",\n",
    "                 config_filenames=[\"base_config_1.yaml\",\n",
    "                                   \"base_config_2.yaml\"],\n",
    "                 random_seeds=[0, 1],\n",
    "                 experiment_dir=\"logs_queue\",\n",
    "                 job_arguments={\"env_name\": \"mle-toolbox\"},\n",
    "                 slack_user_name=os.environ[\"SLACK_USER_NAME\"],\n",
    "                 slack_auth_token=os.environ[\"SLACK_AUTH_TOKEN\"])\n",
    "queue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34745512-a550-4f01-839f-e2eeb4237e7e",
   "metadata": {},
   "source": [
    "![](https://github.com/mle-infrastructure/mle-scheduler/blob/main/docs/slackbot_output.png?raw=true)\n",
    "![](../docs/slackbot_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bce58fc-7f08-4226-a6a9-e5cbee9896a9",
   "metadata": {},
   "source": [
    "# Automatic Log Merging & Plotting with `mle-logging`\n",
    "\n",
    "The `mle-scheduler` is part of the broader `mle-infrastructure`. It therefore has some synergetic features with other packages such as the already mentioned `mle-logging` or `mle-hyperopt`. One of such features is automatic log aggregation after successful completion of all random seeds in `MLEQueue`. All you have to do is supply the option `automerge_seeds`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0f444-e7d4-4f42-a32a-f50351cdb052",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = MLEQueue(resource_to_run=\"local\",\n",
    "                 job_filename=\"train.py\",\n",
    "                 config_filenames=[\"base_config_1.yaml\",\n",
    "                                   \"base_config_2.yaml\"],\n",
    "                 random_seeds=[0, 1],\n",
    "                 experiment_dir=\"logs_merge\",\n",
    "                 automerge_seeds=True)\n",
    "queue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ced8c-d2be-4fb1-bfdb-a5e5be24e8d1",
   "metadata": {},
   "source": [
    "You can then easily load and visualize the results as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18903d-d0a8-49d1-a741-d03d3736ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mle_logging import load_log\n",
    "import datetime\n",
    "date = datetime.datetime.today().strftime(\"%Y-%m-%d\")[2:]\n",
    "log = load_log(f\"logs_merge/{date}_base_config_1\")\n",
    "log.plot([\"train_loss\", \"test_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533fe36d-4602-492d-bc5f-4297e38c0e9e",
   "metadata": {},
   "source": [
    "# Search Jobs with `mle-hyperopt` Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669cf107-e773-4007-b309-52eecca1527b",
   "metadata": {},
   "source": [
    "Finally, `mle-scheduler` integrates smoothly with the hyperparameter search tool `mle-hyperopt`. After defining a search strategy, `mle-hyperopt` allows us to get a set of candidate hyperparameters and to export them as configuration files in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb66ba4-ce5c-40c2-81c9-d438adf022aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import mle_hyperopt\n",
    "except:\n",
    "    !pip install -q mle-hyperopt\n",
    "    import mle_hyperopt\n",
    "\n",
    "from mle_hyperopt import RandomSearch\n",
    "\n",
    "# Instantiate random search class\n",
    "strategy = RandomSearch(real={\"lrate\": {\"begin\": 0.1,\n",
    "                                        \"end\": 0.5,\n",
    "                                        \"prior\": \"log-uniform\"}},\n",
    "                        integer={\"batch_size\": {\"begin\": 1,\n",
    "                                                \"end\": 5,\n",
    "                                                \"prior\": \"uniform\"}},\n",
    "                        categorical={\"arch\": [\"mlp\", \"cnn\"]})\n",
    "\n",
    "# Simple ask - eval - tell API\n",
    "configs, config_fnames = strategy.ask(2, store=True)\n",
    "print(f\"MLE-Hyperopt Stored Configurations: {config_fnames}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8877169-8b2a-4760-8327-1e0affcc6ce0",
   "metadata": {},
   "source": [
    "Next we simply forward the configuration filenames to the `MLEQueue` and voilà we have a batch of search configurations running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea5b65-ee26-42a7-bf35-03addd3ec8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = MLEQueue(resource_to_run=\"local\",\n",
    "                 job_filename=\"train.py\",\n",
    "                 config_filenames=config_fnames,\n",
    "                 experiment_dir=\"logs_search\")\n",
    "queue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d72454b-4c30-4fa5-a509-e30cb0c869a5",
   "metadata": {},
   "source": [
    "Give it a try and let me know what you think! If you find a bug or are missing your favourite feature, feel free to contact me [@RobertTLange](https://twitter.com/RobertTLange) or create an issue!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (mle-toolbox)",
   "language": "python",
   "name": "mle-toolbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
