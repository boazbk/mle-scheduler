{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f9d077-64ed-4dc3-b772-498569e96d2b",
   "metadata": {
    "tags": []
   },
   "source": [
    "# `mle-scheduler`: Lightweight Cluster/Cloud Job Scheduling ðŸš‚\n",
    "### Author: [@RobertTLange](https://twitter.com/RobertTLange) [Last Update: November 2021][![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RobertTLange/mle-scheduler/blob/main/examples/getting_started.ipynb)\n",
    "\n",
    "I really dislike having to write Slurm job submission files. It is tedious, I always forget something and copying old templates feelds cumbersome. What if instead there was a tool that would completely get rid of this manual work? A tool that would schedule jobs on different cluster and cloud resources with a minimal and intuitive job/resource description? The `mle-scheduler` package aims to exactly provide such a service for Slurm, OpenGridEngine and Google Cloud Platform VMs. But you can also use the scheduling utilities to launch multiple training runs on a local machine or on a set of SSH servers. It comes with two core functional pillars:\n",
    "\n",
    "- `MLEJob`: Launches and monitors a single job on a resource (Slurm, OpenGridEngine, GCP, etc.).\n",
    "- `MLEQueue`: Launches and monitors a queue of jobs with different training configurations or random seeds.\n",
    "\n",
    "![](https://github.com/mle-infrastructure/mle-scheduler/blob/main/docs/mle_scheduler_structure.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ff5d35-b29e-4386-b423-9c83ad1ae169",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "try:\n",
    "    import mle_scheduler\n",
    "except:\n",
    "    !pip install -q mle-scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d0799-a069-48ad-a8c7-0f06f5a239c6",
   "metadata": {},
   "source": [
    "Let's start by setting up a simple problem. In the code snippet below we \"pseudo\"-train a network for 10 epochs and log the training run statistics using [`mle-logging`](https://github.com/mle-infrastructure/mle-logging). I like it - but you can use whatever logging setup you prefer. Importantly, there are three command line arguments that are being parsed at the start-up: `experiment_dir` (the main log directory), `config_fname` (the configuration file to be loaded), `seed_id` (the random number seed for reproduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cfa9ca-559b-487e-880e-de865279b353",
   "metadata": {},
   "source": [
    "```python\n",
    "from mle_logging import MLELogger, load_config\n",
    "\n",
    "\n",
    "def main(experiment_dir: str, config_fname: str, seed_id: int):\n",
    "    \"\"\"Example training 'loop' using MLE-Logging.\"\"\"\n",
    "    train_config = load_config(config_fname)\n",
    "    log = MLELogger(\n",
    "        experiment_dir=experiment_dir,\n",
    "        config_fname=config_fname,\n",
    "        seed_id=seed_id,\n",
    "        time_to_track=[\"num_epochs\"],\n",
    "        what_to_track=[\"train_loss\", \"test_loss\"],\n",
    "    )\n",
    "    for epoch in range(1, 11):\n",
    "        train_loss, test_loss = train_your_net(epoch, seed_id, **train_config)\n",
    "        log.update(\n",
    "            {\"num_epochs\": epoch},\n",
    "            {\"train_loss\": train_loss, \"test_loss\": test_loss},\n",
    "            save=True,\n",
    "        )\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Let's train a network.\")\n",
    "    parser.add_argument(\"-exp_dir\", \"--experiment_dir\", type=str)\n",
    "    parser.add_argument(\"-config\", \"--config_fname\", type=str)\n",
    "    parser.add_argument(\"-seed\", \"--seed_id\", type=int, default=1)\n",
    "    args = vars(parser.parse_args())\n",
    "    main(args[\"experiment_dir\"], args[\"config_fname\"], args[\"seed_id\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b0522-6c4b-440d-8dca-345ba9e8a461",
   "metadata": {},
   "source": [
    "# Single Job Management with `MLEJob` ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a221b5-758b-491b-b206-65c3bddeeddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:42:35] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> PID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">40404</span> - Local job scheduled - base_config_1.yaml          <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:146</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fb3800c2310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:42:37] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> PID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">40404</span> - Local job completed - base_config_1.yaml          <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:202</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fb3716adfa0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "from mle_scheduler import MLEJob\n",
    "\n",
    "# Launch a python train_mnist.py -config base_config.json job\n",
    "job = MLEJob(resource_to_run=\"local\",\n",
    "             job_filename=\"train.py\",\n",
    "             config_filename=\"base_config_1.yaml\",\n",
    "             experiment_dir=\"logs_single\",\n",
    "             logger_level=logging.INFO)\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229ca15b-c9a6-4807-8e81-2cc523369a58",
   "metadata": {
    "tags": []
   },
   "source": [
    "So how can you specify the resources that you would like to request/schedule for a job? This happens via the `job_arguments` option. It is a dictionary and depending on the resource you would like to run your job on, you have the following options: \n",
    "\n",
    "| Local | Slurm           | SGE | GCP |\n",
    "|----|----------------------- | ----------- | --------------- |\n",
    "|`env_name` | \n",
    "\n",
    "You can also manually monitor the status of the a scheduled job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e191f835-c110-42f8-bbec-44857399fa1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> PID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">40409</span> - Local job scheduled - base_config_1.yaml          <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:146</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fb3715e57f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[14:42:40] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> PID: <span style=\"color: #000080; text-decoration-color: #000080; font-weight: bold\">40409</span> - Local job completed - base_config_1.yaml          <a href=\"file:///Users/rob/Dropbox/core-code/mle-infrastructure/mle-scheduler/mle_scheduler/job.py\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">job.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:202</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<rich.jupyter.JupyterRenderable at 0x7fb380227910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "job_id = job.schedule()\n",
    "while True:\n",
    "    status = job.monitor(job_id)\n",
    "    if status == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74100d5-a957-4a7e-ae87-4a4626babdc4",
   "metadata": {},
   "source": [
    "# Job Queue Management with `MLEQueue` ðŸš€ðŸš€ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed9e8a7-74fd-4725-9bd6-31106018069d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:02<00:00,  1.92it/s]     \n"
     ]
    }
   ],
   "source": [
    "from mle_scheduler import MLEQueue\n",
    "\n",
    "# Launch a queue of 4 jobs (2 configs x 2 seeds)\n",
    "# python train.py -config base_config_1.yaml -seed 0 -exp_dir logs_queue/<date>_base_config_1\n",
    "# python train.py -config base_config_1.yaml -seed 1 -exp_dir logs_queue/<date>_base_config_1\n",
    "# python train.py -config base_config_2.yaml -seed 0 -exp_dir logs_queue/<date>_base_config_2\n",
    "# python train.py -config base_config_2.yaml -seed 1 -exp_dir logs_queue/<date>_base_config_2\n",
    "queue = MLEQueue(resource_to_run=\"local\",\n",
    "                 job_filename=\"train.py\",\n",
    "                 config_filenames=[\"base_config_1.yaml\",\n",
    "                                   \"base_config_2.yaml\"],\n",
    "                 random_seeds=[0, 1],\n",
    "                 experiment_dir=\"logs_queue\")\n",
    "queue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1587b0-79cb-458b-b4b6-b6d062ade051",
   "metadata": {},
   "source": [
    "You can also control how many jobs to schedule at a given point in time. E.g. if you don't want more than 10 jobs to be running/queued at a time simply add the `max_running_jobs` option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d42168d3-3f66-4c80-b511-269236d5f42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:06<00:00,  1.52s/it]     \n"
     ]
    }
   ],
   "source": [
    "queue = MLEQueue(resource_to_run=\"local\",\n",
    "                 job_filename=\"train.py\",\n",
    "                 config_filenames=[\"base_config_1.yaml\",\n",
    "                                   \"base_config_2.yaml\"],\n",
    "                 random_seeds=[0, 1],\n",
    "                 experiment_dir=\"logs_queue\",\n",
    "                 max_running_jobs=2)\n",
    "queue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62303fa-2961-423c-ac52-22c19e9deeea",
   "metadata": {},
   "source": [
    "# Slack Notification for Real-Time Queue Status\n",
    "\n",
    "You can also get a Slack Bot notification of the current status of the queue. This is powered by the [`slack-clusterbot`](https://github.com/sprekelerlab/slack-clusterbot) package and you can follow these instructions to setup a bot for your favorite workspace [here](https://github.com/sprekelerlab/slack-clusterbot/wiki/Installation). After you have obtained your authentication token it is as simple as providing the token and your user name as inputs. Below you can find the slack chat output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26418d40-b771-4ed1-ba50-0e5826bcfc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:03<00:00,  1.04it/s]     \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "queue = MLEQueue(resource_to_run=\"local\",\n",
    "                 job_filename=\"train.py\",\n",
    "                 config_filenames=[\"base_config_1.yaml\",\n",
    "                                   \"base_config_2.yaml\"],\n",
    "                 random_seeds=[0, 1],\n",
    "                 experiment_dir=\"logs_queue\",\n",
    "                 job_arguments={\"env_name\": \"mle-toolbox\"},\n",
    "                 slack_user_name=os.environ[\"SLACK_USER_NAME\"],\n",
    "                 slack_auth_token=os.environ[\"SLACK_AUTH_TOKEN\"])\n",
    "queue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34745512-a550-4f01-839f-e2eeb4237e7e",
   "metadata": {},
   "source": [
    "![](https://github.com/mle-infrastructure/mle-scheduler/blob/main/docs/slackbot_output.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bce58fc-7f08-4226-a6a9-e5cbee9896a9",
   "metadata": {},
   "source": [
    "# Automatic Log Merging & Plotting with `mle-logging`\n",
    "\n",
    "The `mle-scheduler` is part of the broader `mle-infrastructure`. It therefore has some synergetic features with other packages such as the already mentioned `mle-logging` or `mle-hyperopt`. One of such features is automatic log aggregation after successful completion of all random seeds in `MLEQueue`. All you have to do is supply the option `automerge_seeds`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e0f444-e7d4-4f42-a32a-f50351cdb052",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = MLEQueue(resource_to_run=\"local\",\n",
    "                 job_filename=\"train.py\",\n",
    "                 config_filenames=[\"base_config_1.yaml\",\n",
    "                                   \"base_config_2.yaml\"],\n",
    "                 random_seeds=[0, 1],\n",
    "                 experiment_dir=\"logs_queue2\",\n",
    "                 automerge_seeds=True)\n",
    "queue.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ced8c-d2be-4fb1-bfdb-a5e5be24e8d1",
   "metadata": {},
   "source": [
    "You can then easily load and visualize the results as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18903d-d0a8-49d1-a741-d03d3736ea7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mle_logging import load_log\n",
    "\n",
    "log = load_log(\"logs_queue2/21-11-07_base_config_1\")\n",
    "log.plot([\"train_loss\", \"test_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533fe36d-4602-492d-bc5f-4297e38c0e9e",
   "metadata": {},
   "source": [
    "# Search Jobs with `mle-hyperopt` Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669cf107-e773-4007-b309-52eecca1527b",
   "metadata": {},
   "source": [
    "Finally, `mle-scheduler` integrates smoothly with the hyperparameter search tool `mle-hyperopt`. After defining a search strategy, `mle-hyperopt` allows us to get a set of candidate hyperparameters and to export them as configuration files in one go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb66ba4-ce5c-40c2-81c9-d438adf022aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import mle_hyperopt\n",
    "except:\n",
    "    !pip install -q mle-hyperopt\n",
    "    import mle_hyperopt\n",
    "\n",
    "from mle_hyperopt import RandomSearch\n",
    "\n",
    "# Instantiate random search class\n",
    "strategy = RandomSearch(real={\"lrate\": {\"begin\": 0.1,\n",
    "                                        \"end\": 0.5,\n",
    "                                        \"prior\": \"log-uniform\"}},\n",
    "                        integer={\"batch_size\": {\"begin\": 1,\n",
    "                                                \"end\": 5,\n",
    "                                                \"prior\": \"uniform\"}},\n",
    "                        categorical={\"arch\": [\"mlp\", \"cnn\"]})\n",
    "\n",
    "# Simple ask - eval - tell API\n",
    "configs, config_fnames = strategy.ask(2, store=True)\n",
    "config_fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8877169-8b2a-4760-8327-1e0affcc6ce0",
   "metadata": {},
   "source": [
    "Next we simply forward the configuration filenames to the `MLEQueue` and voilÃ  we have a batch of search configurations running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea5b65-ee26-42a7-bf35-03addd3ec8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = MLEQueue(resource_to_run=\"local\",\n",
    "                 job_filename=\"train.py\",\n",
    "                 config_filenames=config_fnames,\n",
    "                 experiment_dir=\"logs_queue\")\n",
    "queue.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (mle-toolbox)",
   "language": "python",
   "name": "mle-toolbox"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
